import streamlit as st
import pandas as pd
import io
import requests
from PIL import Image
from datetime import datetime
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.pagesizes import A4
from reportlab.pdfbase.pdfmetrics import stringWidth
from reportlab.lib.utils import ImageReader
import time
from pathlib import Path
from reportlab.pdfbase.cidfonts import UnicodeCIDFont
import re

# ---- ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆIPAex ã‚’å„ªå…ˆã€ç„¡ã‘ã‚Œã°CIDãƒ•ã‚©ãƒ³ãƒˆã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰----
def _setup_font():
    here = Path(__file__).parent
    candidates = [
        here / "fonts" / "IPAexGothic.ttf",
        here / "IPAexGothic.ttf",
        Path.cwd() / "fonts" / "IPAexGothic.ttf",
        Path.cwd() / "IPAexGothic.ttf",
    ]
    for p in candidates:
        if p.exists():
            pdfmetrics.registerFont(TTFont("Japanese", str(p)))
            return "Japanese"
    pdfmetrics.registerFont(UnicodeCIDFont("HeiseiKakuGo-W5"))
    return "HeiseiKakuGo-W5"

JAPANESE_FONT = _setup_font()

st.set_page_config(page_title="ğŸ” å­¦ç”ŸæŒ‡å°ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", layout="wide")
st.title("ğŸ” QuoVadisæ­¯ç§‘åŒ»å¸«å›½å®¶è©¦é¨“ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹")

# ===== åˆ—åæ­£è¦åŒ– & å®‰å…¨å–å¾—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    """BOM/ç©ºç™½/æ”¹è¡Œã‚’é™¤å»ã—ã€ã‚ˆãã‚ã‚‹åˆ¥åã‚’æ­£å¼åã¸å¯„ã›ã‚‹"""
    def _clean(s):
        s = str(s).replace("\ufeff", "")
        return re.sub(r"[\u3000 \t\r\n]+", "", s)
    df = df.copy()
    df.columns = [_clean(c) for c in df.columns]

    alias = {
        "å•é¡Œæ–‡":  ["è¨­å•", "å•é¡Œ", "æœ¬æ–‡"],
        "é¸æŠè‚¢1": ["é¸æŠè‚¢ï¼¡","é¸æŠè‚¢a","A","ï½"],
        "é¸æŠè‚¢2": ["é¸æŠè‚¢ï¼¢","é¸æŠè‚¢b","B","ï½‚"],
        "é¸æŠè‚¢3": ["é¸æŠè‚¢ï¼£","é¸æŠè‚¢c","C","ï½ƒ"],
        "é¸æŠè‚¢4": ["é¸æŠè‚¢ï¼¤","é¸æŠè‚¢d","D","ï½„"],
        "é¸æŠè‚¢5": ["é¸æŠè‚¢ï¼¥","é¸æŠè‚¢e","E","ï½…"],
        "æ­£è§£":    ["è§£ç­”","ç­”ãˆ","ans","answer"],
        "ç§‘ç›®åˆ†é¡": ["åˆ†é¡","ç§‘ç›®","ã‚«ãƒ†ã‚´ãƒª","ã‚«ãƒ†ã‚´ãƒªãƒ¼"],
        "ãƒªãƒ³ã‚¯URL": ["ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","ç”»åƒLink"],
    }
    colset = set(df.columns)
    for canon, cands in alias.items():
        if canon in colset:
            continue
        for c in cands:
            if c in colset:
                df.rename(columns={c: canon}, inplace=True)
                colset.add(canon)
                break
    return df

def safe_get(row: pd.Series | dict, keys, default=""):
    """Series/è¾æ›¸ã‹ã‚‰å®‰å…¨ã«å€¤ã‚’å–å¾—ï¼ˆNaN, ç©ºç™½, åˆ¥åã‚’è€ƒæ…®ï¼‰"""
    if isinstance(row, pd.Series):
        row = row.to_dict()
    for k in keys:
        if k in row:
            v = row.get(k)
            try:
                if pd.isna(v):
                    continue
            except Exception:
                pass
            s = str(v).strip() if v is not None else ""
            if s:
                return s
    return default

def ensure_output_columns(df: pd.DataFrame) -> pd.DataFrame:
    need = ["å•é¡Œæ–‡","é¸æŠè‚¢1","é¸æŠè‚¢2","é¸æŠè‚¢3","é¸æŠè‚¢4","é¸æŠè‚¢5","æ­£è§£","ç§‘ç›®åˆ†é¡","ãƒªãƒ³ã‚¯URL"]
    out = df.copy()
    for c in need:
        if c not in out.columns:
            out[c] = ""
    return out

# ===== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ =====
# BOM å¯¾ç­–ã®ãŸã‚ utf-8-sigã€æ–‡å­—åˆ—ã§çµ±ä¸€ã—ã¦å–ã‚Šè¾¼ã¿
df = pd.read_csv("97_118DB.csv", dtype=str, encoding="utf-8-sig")
df = df.fillna("")
df = normalize_columns(df)

# ===== æ¤œç´¢ =====
query = st.text_input("å•é¡Œæ–‡ãƒ»é¸æŠè‚¢ãƒ»åˆ†é¡ãƒ»ç”»åƒãƒªãƒ³ã‚¯(URL)ã§æ¤œç´¢:")
st.caption("ğŸ’¡ æ¤œç´¢èªã‚’ `&` ã§ã¤ãªã’ã‚‹ã¨ANDæ¤œç´¢ï¼ˆä¾‹: ãƒ¬ã‚¸ãƒ³ & ç¡¬ã•ï¼‰ã€‚URLã®ä¸€éƒ¨ï¼ˆä¾‹: http, drive.googleï¼‰ã§ã‚‚å¯ã€‚")

if not query:
    st.stop()

keywords = [kw.strip() for kw in query.split("&") if kw.strip()]

def row_text(r: pd.Series) -> str:
    # ğŸ”¸ ã“ã“ã‚’å¤‰æ›´ï¼šãƒªãƒ³ã‚¯ç³»ã‚«ãƒ©ãƒ ã‚‚æ¤œç´¢å¯¾è±¡ã«å«ã‚ã‚‹
    parts = [
        safe_get(r, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"]),
        *[safe_get(r, [f"é¸æŠè‚¢{i}"]) for i in range(1,6)],
        safe_get(r, ["æ­£è§£","è§£ç­”","ç­”ãˆ"]),
        safe_get(r, ["ç§‘ç›®åˆ†é¡","åˆ†é¡","ç§‘ç›®"]),
        # è¿½åŠ ï¼šURL/ç”»åƒãƒªãƒ³ã‚¯
        safe_get(r, ["ãƒªãƒ³ã‚¯URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","ç”»åƒLink"]),
    ]
    return " ".join([p for p in parts if p])

df_filtered = df[df.apply(
    lambda row: all(kw.lower() in row_text(row).lower() for kw in keywords),
    axis=1
)]
df_filtered = df_filtered.reset_index(drop=True)

st.info(f"{len(df_filtered)}ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸ")

timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
file_prefix = f"{(query if query else 'æ¤œç´¢ãªã—')}{timestamp}"

# ===== CSV ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====
csv_buffer = io.StringIO()
ensure_output_columns(df_filtered).to_csv(csv_buffer, index=False)
st.download_button(
    label="ğŸ“¥ ãƒ’ãƒƒãƒˆçµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=csv_buffer.getvalue(),
    file_name=f"{file_prefix}.csv",
    mime="text/csv"
)

# --------------------------------------------------------------------
# â–¼â–¼â–¼ ã“ã“ã‹ã‚‰è¿½åŠ ï¼ˆæœ€å°å¤‰æ›´ï¼‰ï¼šGoodNotesç”¨CSVãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‹ãƒœã‚¿ãƒ³ â–¼â–¼â–¼

def _gn_clean(s: str) -> str:
    if s is None:
        return ""
    return str(s).replace("\ufeff", "").strip().replace("ã€€", "")

def _gn_normalize_newlines(text: str, newline: str = "\n") -> str:
    """ã‚»ãƒ«å†…ã®æ”¹è¡Œã‚’LFã«çµ±ä¸€ï¼ˆå¿…è¦ãªã‚‰ CRLF ã¸å†å¤‰æ›ï¼‰"""
    if text is None:
        return ""
    t = re.sub(r"\r\n|\r", "\n", str(text))
    if newline == "\r\n":
        t = t.replace("\n", "\r\n")
    return t

def _gn_make_front_back(row: pd.Series,
                        numbering: str = "ABC",
                        add_labels: bool = True,
                        add_meta: bool = False) -> tuple[str, str]:
    q = _gn_clean(row.get("å•é¡Œæ–‡", ""))

    choices = [
        _gn_clean(row.get("é¸æŠè‚¢1", "")),
        _gn_clean(row.get("é¸æŠè‚¢2", "")),
        _gn_clean(row.get("é¸æŠè‚¢3", "")),
        _gn_clean(row.get("é¸æŠè‚¢4", "")),
        _gn_clean(row.get("é¸æŠè‚¢5", "")),
    ]
    labels = ["A","B","C","D","E"] if numbering == "ABC" else ["1","2","3","4","5"]
    choice_lines = [f"{labels[i]}. {_gn_normalize_newlines(txt)}" for i, txt in enumerate(choices) if txt]

    front = _gn_normalize_newlines(q)
    if choice_lines:
        front = front + "\n\n" + "\n".join(choice_lines)

    ans = _gn_clean(row.get("æ­£è§£", ""))
    back = f"æ­£è§£: {ans}" if add_labels else ans

    if add_meta:
        subject = _gn_clean(row.get("ç§‘ç›®åˆ†é¡",""))
        link = _gn_clean(row.get("ãƒªãƒ³ã‚¯URL",""))
        extra = "\n".join([s for s in (subject, link) if s])
        if extra:
            back = back + "\n\n" + _gn_normalize_newlines(extra)

    back = _gn_normalize_newlines(back)
    return front, back

def dataframe_to_goodnotes_bytes(df: pd.DataFrame,
                                 numbering: str = "ABC",
                                 add_labels: bool = True,
                                 add_meta: bool = False,
                                 overall_line_ending: str = "lf",
                                 quote_all: bool = False) -> bytes:
    """
    ä»»æ„ã® DataFrame ã‹ã‚‰ GoodNotes ç”¨ Front/Back CSV ã‚’ UTF-8(BOMä»˜ã) bytes ã§è¿”ã™ã€‚
    - ã‚»ãƒ«å†…éƒ¨ã®æ”¹è¡Œã¯ LF ã«æ­£è¦åŒ–ï¼ˆGoodNotesã§ã®è¡¨ç¤ºå®‰å®šã®ãŸã‚ï¼‰
    - ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®æ”¹è¡Œã¯ overall_line_ending ã§ 'lf' or 'crlf'
    """
    # å¿…è¦åˆ—ã®æ‹…ä¿ï¼ˆãªã‘ã‚Œã°ç©ºåˆ—ã‚’è¶³ã™ï¼‰
    base = ensure_output_columns(df)

    fronts, backs = [], []
    for _, row in base.iterrows():
        f, b = _gn_make_front_back(row, numbering=numbering, add_labels=add_labels, add_meta=add_meta)
        fronts.append(f); backs.append(b)

    out = pd.DataFrame({"Front": fronts, "Back": backs})

    # ã‚»ãƒ«å†…éƒ¨ã®æ”¹è¡Œã‚’LFã¸çµ±ä¸€
    for c in out.columns:
        out[c] = out[c].map(lambda v: _gn_normalize_newlines(v, "\n"))

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæœ«
    file_nl = "\n" if overall_line_ending.lower() == "lf" else "\r\n"
    import csv as _csv  # æ—¢å­˜importæ±šæŸ“ã‚’é¿ã‘ã‚‹ãŸã‚ãƒ­ãƒ¼ã‚«ãƒ«å‚ç…§

    # pandasã®StringIOã§ã¯encodingå¼•æ•°ãŒç„¡è¦–ã•ã‚Œã‚‹ãŸã‚ã€æ‰‹å‹•ã§BOMã‚’æ›¸ãè¾¼ã‚€
    buf = io.StringIO()
    buf.write("\ufeff")  # BOM
    out.to_csv(
        buf,
        index=False,
        lineterminator=file_nl,
        quoting=_csv.QUOTE_ALL if quote_all else _csv.QUOTE_MINIMAL,
        doublequote=True,
        escapechar="\\",
    )
    return buf.getvalue().encode("utf-8")

# â–¼ GoodNotesãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼ˆæ—¢å­˜CSVãƒœã‚¿ãƒ³ã®ç›´ä¸‹ï¼‰
st.download_button(
    label="ğŸ“¥ GoodNotesç”¨CSVï¼ˆFront/Backï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=dataframe_to_goodnotes_bytes(
        df_filtered,          # æ¤œç´¢çµæœã‚’ãã®ã¾ã¾Front/BackåŒ–
        numbering="ABC",      # "123"ã«ã—ãŸã„å ´åˆã¯ã“ã“ã‚’å¤‰æ›´
        add_labels=True,      # Backå…ˆé ­ã«ã€Œæ­£è§£: ã€ã‚’ä»˜ã‘ã‚‹
        add_meta=False,       # Backæœ«å°¾ã« ç§‘ç›®åˆ†é¡/ãƒªãƒ³ã‚¯URL ã‚’è¿½è¨˜ã™ã‚‹ãªã‚‰ True
        overall_line_ending="lf",  # GoodNotesãªã‚‰LFæ¨å¥¨ï¼ˆWindowsé‹ç”¨ãªã‚‰"crlf"ã‚‚å¯ï¼‰
    ),
    file_name=f"{file_prefix}_goodnotes.csv",
    mime="text/csv",
)
# --------------------------------------------------------------------

# ===== TXT æ•´å½¢ =====
def convert_google_drive_link(url):
    if "drive.google.com" in url and "/file/d/" in url:
        try:
            file_id = url.split("/file/d/")[1].split("/")[0]
            return f"https://drive.google.com/uc?export=view&id={file_id}"
        except Exception:
            return url
    return url

def wrap_text(text: str, max_width: float, font_name: str, font_size: int):
    s = "" if text is None else str(text)
    if s == "":
        return [""]
    lines, buf = [], ""
    for ch in s:
        if stringWidth(buf + ch, font_name, font_size) <= max_width:
            buf += ch
        else:
            lines.append(buf)
            buf = ch
    if buf:
        lines.append(buf)
    return lines

def wrapped_lines(prefix: str, value: str, usable_width: float, font: str, size: int):
    return wrap_text(f"{prefix}{value}", usable_width, font, size)

def format_record_to_text(row: pd.Series) -> str:
    q = safe_get(row, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"])
    parts = [f"å•é¡Œæ–‡: {q}"]
    for i in range(1, 6):
        choice = safe_get(row, [f"é¸æŠè‚¢{i}"])
        if choice:
            parts.append(f"é¸æŠè‚¢{i}: {choice}")
    parts.append(f"æ­£è§£: {safe_get(row, ['æ­£è§£','è§£ç­”','ç­”ãˆ'])}")
    parts.append(f"åˆ†é¡: {safe_get(row, ['ç§‘ç›®åˆ†é¡','åˆ†é¡','ç§‘ç›®'])}")
    link = safe_get(row, ["ãƒªãƒ³ã‚¯URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯","ç”»åƒLink"])
    if link:
        parts.append(f"ç”»åƒãƒªãƒ³ã‚¯: {convert_google_drive_link(link)}ï¼ˆPDFã«ç”»åƒè¡¨ç¤ºï¼‰")
    return "\n".join(parts)

# ===== TXT ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ =====
txt_buffer = io.StringIO()
for _, row in df_filtered.iterrows():
    txt_buffer.write(format_record_to_text(row))
    txt_buffer.write("\n\n" + "-"*40 + "\n\n")
st.download_button(
    label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’TEXTãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data=txt_buffer.getvalue(),
    file_name=f"{file_prefix}.txt",
    mime="text/plain"
)

# ===== PDF ä½œæˆï¼ˆãƒšãƒ¼ã‚¸å…ˆé ­ã¯å¿…ãšå•é¡Œæ–‡ã‹ã‚‰ï¼ç”»åƒã¯å¿…ãšè¡¨ç¤ºï¼‰=====
def create_pdf(records, progress=None, status=None, start_time=None):
    pdf_buffer = io.BytesIO()
    c = canvas.Canvas(pdf_buffer, pagesize=A4)
    c.setFont(JAPANESE_FONT, 12)
    width, height = A4

    top_margin, bottom_margin = 40, 60
    left_margin, right_margin = 40, 40
    usable_width = width - left_margin - right_margin
    page_usable_h = (height - top_margin) - bottom_margin
    line_h = 18
    y = height - top_margin

    total = len(records)

    def fmt(sec):
        m = int(sec // 60); s = int(sec % 60)
        return f"{m:02d}:{s:02d}"

    def new_page():
        nonlocal y
        c.showPage()
        c.setFont(JAPANESE_FONT, 12)
        y = height - top_margin

    def draw_wrapped_lines(lines):
        nonlocal y
        for ln in lines:
            c.drawString(left_margin, y, ln)
            y -= line_h

    for idx, (_, row) in enumerate(records.iterrows(), start=1):
        q = safe_get(row, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"])

        # é¸æŠè‚¢
        choices = []
        for i in range(1, 6):
            v = safe_get(row, [f"é¸æŠè‚¢{i}"])
            if v:
                choices.append((i, v))

        ans = safe_get(row, ["æ­£è§£","è§£ç­”","ç­”ãˆ"])
        cat = safe_get(row, ["ç§‘ç›®åˆ†é¡","åˆ†é¡","ç§‘ç›®"])

        # ç”»åƒã®äº‹å‰å–å¾—
        pil = None
        img_est_h = 0
        link_raw = safe_get(row, ["ãƒªãƒ³ã‚¯URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯"])
        if link_raw:
            try:
                image_url = convert_google_drive_link(link_raw)
                resp = requests.get(image_url, timeout=5)
                pil = Image.open(io.BytesIO(resp.content)).convert("RGB")
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                img_est_h = nh + 20
            except Exception:
                pil = None
                img_est_h = wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12)
                img_est_h = len(img_est_h) * line_h

        # é«˜ã•è¦‹ç©ã‚Š
        est_h = 0
        q_lines = wrapped_lines("å•é¡Œæ–‡: ", q, usable_width, JAPANESE_FONT, 12)
        est_h += len(q_lines) * line_h
        choice_lines_list = []
        for i, v in choices:
            ls = wrapped_lines(f"é¸æŠè‚¢{i}: ", v, usable_width, JAPANESE_FONT, 12)
            choice_lines_list.append(ls)
            est_h += len(ls) * line_h
        est_h += img_est_h if img_est_h else 0
        ans_lines = wrapped_lines("æ­£è§£: ", ans, usable_width, JAPANESE_FONT, 12)
        cat_lines = wrapped_lines("åˆ†é¡: ", cat, usable_width, JAPANESE_FONT, 12)
        est_h += len(ans_lines) * line_h + len(cat_lines) * line_h + 20

        # ãƒšãƒ¼ã‚¸å…ˆé ­ã‚’å¿…ãšå•é¡Œæ–‡ã‹ã‚‰
        if y - est_h < bottom_margin:
            new_page()

        # æç”»
        draw_wrapped_lines(q_lines)
        for ls in choice_lines_list:
            draw_wrapped_lines(ls)

        if pil is not None:
            try:
                iw, ih = pil.size
                scale = min(usable_width / iw, page_usable_h / ih, 1.0)
                nw, nh = iw * scale, ih * scale
                if y - nh < bottom_margin:
                    new_page()
                remaining = y - bottom_margin
                if nh > remaining:
                    adj = remaining / nh
                    nw, nh = nw * adj, nh * adj
                img_io = io.BytesIO()
                pil.save(img_io, format="PNG")
                img_io.seek(0)
                img_reader = ImageReader(img_io)
                c.drawImage(img_reader, left_margin, y - nh, width=nw, height=nh, preserveAspectRatio=True, mask='auto')
                y -= nh + 20
            except Exception as e:
                err_lines = wrapped_lines("", f"[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {e}]", usable_width, JAPANESE_FONT, 12)
                draw_wrapped_lines(err_lines)
        else:
            if link_raw:
                draw_wrapped_lines(wrapped_lines("", "[ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—]", usable_width, JAPANESE_FONT, 12))

        draw_wrapped_lines(ans_lines)
        draw_wrapped_lines(cat_lines)

        if y - 20 < bottom_margin:
            new_page()
        else:
            y -= 20

        if st.session_state.get("progress_on"):
            st.session_state["progress"].progress(min(idx / max(total, 1), 1.0))

    c.save()
    pdf_buffer.seek(0)
    return pdf_buffer.getvalue()

# ===== PDF ç”Ÿæˆ =====
if "pdf_bytes" not in st.session_state:
    st.session_state["pdf_bytes"] = None

if st.button("ğŸ–¨ï¸ PDFã‚’ä½œæˆï¼ˆç”»åƒä»˜ãï¼‰"):
    st.session_state["progress_on"] = True
    st.session_state["progress"] = st.progress(0.0)
    start = time.time()
    with st.spinner("PDFã‚’ä½œæˆä¸­â€¦"):
        st.session_state["pdf_bytes"] = create_pdf(df_filtered)
    st.session_state["progress_on"] = False
    st.success("âœ… PDFä½œæˆå®Œäº†ï¼")

if st.session_state["pdf_bytes"] is not None:
    st.download_button(
        label="ğŸ“„ ãƒ’ãƒƒãƒˆçµæœã‚’PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=st.session_state["pdf_bytes"],
        file_name=f"{file_prefix}.pdf",
        mime="application/pdf"
    )

# ===== ç”»é¢ã®ä¸€è¦§ï¼ˆæ­£è§£ã¯åˆæœŸéè¡¨ç¤ºï¼‰=====
st.markdown("### ğŸ” ãƒ’ãƒƒãƒˆã—ãŸå•é¡Œä¸€è¦§")
for i, (_, record) in enumerate(df_filtered.iterrows()):
    title = safe_get(record, ["å•é¡Œæ–‡","è¨­å•","å•é¡Œ","æœ¬æ–‡"])
    with st.expander(f"{i+1}. {title[:50]}..."):
        st.markdown("### ğŸ“ å•é¡Œæ–‡")
        st.write(title)

        st.markdown("### âœï¸ é¸æŠè‚¢")
        for j in range(1, 6):
            val = safe_get(record, [f"é¸æŠè‚¢{j}"])
            if val:
                st.write(f"- {val}")

        show_ans = st.checkbox("æ­£è§£ã‚’è¡¨ç¤ºã™ã‚‹", key=f"show_answer_{i}", value=False)
        if show_ans:
            st.markdown(f"**âœ… æ­£è§£:** {safe_get(record, ['æ­£è§£','è§£ç­”','ç­”ãˆ'])}")
        else:
            st.markdown("**âœ… æ­£è§£:** |||ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§è¡¨ç¤ºï¼‰|||")

        st.markdown(f"**ğŸ“š åˆ†é¡:** {safe_get(record, ['ç§‘ç›®åˆ†é¡','åˆ†é¡','ç§‘ç›®'])}")

        link = safe_get(record, ["ãƒªãƒ³ã‚¯URL","ç”»åƒURL","ç”»åƒãƒªãƒ³ã‚¯","ãƒªãƒ³ã‚¯"])
        if link:
            st.markdown(f"[ç”»åƒãƒªãƒ³ã‚¯ã¯ã“ã¡ã‚‰]({convert_google_drive_link(link)})")
        else:
            st.write("ï¼ˆç”»åƒãƒªãƒ³ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰")

# ãƒ‡ãƒãƒƒã‚°è£œåŠ©ï¼ˆå¿…è¦æ™‚ã ã‘å±•é–‹ï¼‰
#with st.expander("ğŸ”§ ç¾åœ¨ã®åˆ—åï¼ˆæ­£è¦åŒ–å¾Œï¼‰"):
#   st.write(list(df.columns))
